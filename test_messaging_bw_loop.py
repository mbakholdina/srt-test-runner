""" 
Bandwidth loop test.

A script that runs 1 receiver and N senders (concurrently, in parallel) 
which stream with a specified bitrate. This is done iteratively every time
with a different value of bitrate what helps to determine a maximum 
availabale bandwidth at the moment of running the script.
"""
import concurrent.futures
import configparser
import logging
import pathlib
import signal
import shutil
import subprocess
import sys
import time
import typing

import attr
import click
import fabric

import shared


# TODO:     Add an option to download stats and Wireshark dumps via scp (fabric),
#           Adjust time and the process of running N senders concurrently,
#           Test the script on Windows with regard to ssh-agent,
#           Disbale password promt (fabric),
#           Fix the problem with carriage (\r\n) generated by pseudo-terminal,
#           Add running tshark remotely on a receiver side,
#           Add running sender side application remotely (?),
#           Improve documentation,
#           Disable warnings from paramiko


logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)-15s [%(levelname)s] %(message)s',
)
logger = logging.getLogger(__name__)


def start_sender(
    snd_path_to_srt,
    dst_host,
    dst_port,
    params,
    results_dir,
    collect_stats: bool=False,
    file_info=None,
    sender_number=None
):
    name = f'srt sender {sender_number}'
    logger.info(f'Starting on a local machine: {name}')

    bitrate, repeat, maxbw = params
    args = []
    args += [
        f'{snd_path_to_srt}/srt-test-messaging', 
        f'srt://{dst_host}:{dst_port}?sndbuf=12058624&smoother=live&maxbw={maxbw}',
        "",
        '-msgsize', '1456',
        '-reply', '0', 
        '-printmsg', '0',
        '-bitrate', str(bitrate), 
        '-repeat', str(repeat),
    ]
    if collect_stats:
        scenario, algdescr, bitrate = file_info
        # FIXME: Create results folder automatically
        stats_file = results_dir / f'{scenario}-alg-{algdescr}-blt-{bitrate / shared.DELIMETER}Mbps-stats-snd-{sender_number}.csv'
        args += [
            '-statsfreq', '1',
            '-statsfile', stats_file,
        ]
        
    snd_srt_process = shared.create_process(name, args)
    logger.info(f'Started successfully: {name}')
    return (name, snd_srt_process)

def start_receiver(
    rcv_ssh_host, 
    rcv_ssh_username, 
    rcv_path_to_srt, 
    dst_port,
    results_dir,
    collect_stats: bool=False,
    file_info=None
):
    # FIXME: maxcon=50 is hard-coded for now
    name = 'srt receiver'
    logger.info(f'Starting {name} on a remote machine: {rcv_ssh_host}')
    args = []
    args += shared.SSH_COMMON_ARGS
    args += [
        f'{rcv_ssh_username}@{rcv_ssh_host}',
        f'{rcv_path_to_srt}/srt-test-messaging',
        f'"srt://:{dst_port}?rcvbuf=12058624&smoother=live&maxcon=50"',
        '-msgsize', '1456',
        '-reply', '0', 
        '-printmsg', '0'
    ]
    if collect_stats:
        args += ['-statsfreq', '1']
        scenario, algdescr, bitrate = file_info
        stats_file = results_dir / f'{scenario}-alg-{algdescr}-blt-{bitrate / shared.DELIMETER}Mbps-stats-rcv.csv'
        args += ['-statsfile', stats_file]
    process = shared.create_process(name, args, True)
    logger.info('Started successfully')
    return (name, process)

@attr.s
class Config:
    """
    Global configuration settings.
    """
    rcv_ssh_host: str = attr.ib()
    rcv_ssh_username: str = attr.ib()
    rcv_path_to_srt: str = attr.ib()
    snd_path_to_srt: str = attr.ib()
    snd_tshark_iface: str = attr.ib()
    dst_host: str = attr.ib()
    dst_port: str = attr.ib()
    algdescr: str = attr.ib()
    scenario: str = attr.ib()
    bitrate_min: int = attr.ib()
    bitrate_max: int = attr.ib()
    bitrate_step: int = attr.ib()
    time_to_stream: int = attr.ib()

    @classmethod
    def from_config_filepath(cls, config_filepath: pathlib.Path):
        parsed_config = configparser.ConfigParser()
        with config_filepath.open('r', encoding='utf-8') as fp:
            parsed_config.read_file(fp)
        return cls(
            parsed_config['receiver']['rcv_ssh_host'],
            parsed_config['receiver']['rcv_ssh_username'],
            parsed_config['receiver']['rcv_path_to_srt'],
            parsed_config['sender']['snd_path_to_srt'],
            parsed_config['sender']['snd_tshark_iface'],
            parsed_config['bw-loop-test']['dst_host'],
            parsed_config['bw-loop-test']['dst_port'],
            parsed_config['bw-loop-test']['algdescr'],
            parsed_config['bw-loop-test']['scenario'],
            int(parsed_config['bw-loop-test']['bitrate_min']),
            int(parsed_config['bw-loop-test']['bitrate_max']),
            int(parsed_config['bw-loop-test']['bitrate_step']),
            int(parsed_config['bw-loop-test']['time_to_stream'])
        )

def start_several_senders(
    config,
    bitrate,
    snd_number,
    snd_mode,
    results_dir,
    collect_stats,
    file_info
):
    # Calculate number of packets for time_to_stream sec of streaming
    # based on the target bitrate and packet size
    repeat = config.time_to_stream * bitrate // (1456 * 8)
    maxbw  = int(bitrate // 8 * 1.25)
    params = (bitrate, repeat, maxbw)

    logger.info(
        f'Starting streaming with bitrate {bitrate}, repeat {repeat}, '
        f'senders {snd_number}'
    )

    sender_processes = []

    if snd_number == 1 or snd_mode == 'concurrently':
        for i in range(0, snd_number):
            snd_srt_process = start_sender(
                config.snd_path_to_srt,
                config.dst_host,
                config.dst_port,
                params,
                results_dir,
                collect_stats,
                file_info,
                i
            )
            sender_processes.append(snd_srt_process)

    if snd_mode == 'parallel':
        with concurrent.futures.ThreadPoolExecutor(max_workers=snd_number) as executor:
            # TODO: Change to list (?)
            future_senders = {
                executor.submit(
                    start_sender, 
                    config.snd_path_to_srt, 
                    config.dst_host,
                    config.dst_port, 
                    params, 
                    results_dir,
                    collect_stats, 
                    file_info, 
                    i
                ): i for i in range(0, snd_number)
            }

            errors = 0
            for future in concurrent.futures.as_completed(future_senders):
                try:
                    process = future.result()
                except Exception as exc:
                    logger.info(
                        f'{future_senders[future]} sender generated an '
                        f'exception: {exc}'
                    )
                    errors += 1
                else:
                    sender_processes.append(process)

            if errors > 0:
                raise shared.ParallelSendersExecutionFailed()

    return sender_processes

def main_function(
    config_filepath,
    rcv,
    snd_number,
    snd_mode,
    results_dir,
    collect_stats,
    run_tshark
):
    config = Config.from_config_filepath(pathlib.Path(config_filepath))

    processes = []
    try:
        if rcv == 'remotely':
            logger.info('Creating a folder for storing results on a receiver side')
            # FIXME: By default Paramiko will attempt to connect to a running 
            # SSH agent (Unix style, e.g. a live SSH_AUTH_SOCK, or Pageant if 
            # one is on Windows). That's why promt for login-password is not 
            # disabled under condition that password is not configured via 
            # connect_kwargs.password
            with fabric.Connection(host=config.rcv_ssh_host, user=config.rcv_ssh_username) as c:
                result = c.run(f'rm -rf {results_dir}')
                if result.exited != 0:
                    logger.info(f'Not created: {result}')
                    return
                result = c.run(f'mkdir {results_dir}')
                if result.exited != 0:
                    logger.info(f'Not created: {result}')
                    return
            logger.info('Created successfully')

        logger.info('Creating a folder for saving results on a sender side')
        results_dir = pathlib.Path(results_dir)
        if results_dir.exists():
            shutil.rmtree(results_dir)
        results_dir.mkdir()
        logger.info('Created successfully')

        for bitrate in range(config.bitrate_min, config.bitrate_max, config.bitrate_step):
            # Information needed to form .csv stats and .pcapng WireShark
            # files' names
            file_info = (config.scenario, config.algdescr, bitrate)

            # Start SRT on a receiver side
            if rcv == 'remotely':
                rcv_srt_process = start_receiver(
                    config.rcv_ssh_host, 
                    config.rcv_ssh_username, 
                    config.rcv_path_to_srt, 
                    config.dst_port,
                    results_dir,
                    collect_stats,
                    file_info
                )
                processes.append(rcv_srt_process)
                time.sleep(3)

            # Start tshark on a sender side
            if run_tshark:
                filename = f'{config.scenario}-alg-{config.algdescr}-blt-{bitrate / shared.DELIMETER}Mbps-snd.pcapng'
                snd_tshark_process = shared.start_tshark(
                    config.snd_tshark_iface, 
                    config.dst_port, 
                    filename,
                    results_dir
                )
                processes.append(snd_tshark_process)
                time.sleep(3)

            # Start several SRT senders on a sender side to stream for
            # config.time_to_stream seconds
            sender_processes = start_several_senders(
                config,
                bitrate,
                snd_number,
                snd_mode,
                results_dir,
                collect_stats,
                file_info
            )
            for p in sender_processes:
                processes.append(p)

            # Sleep for config.time_to_stream seconds to wait while senders 
            # will finish the streaming and then check how many senders are 
            # still running.
            # FIXME: Time adjustment is needed for snd_mode='concurrently'
            time.sleep(config.time_to_stream)
            extra_time = shared.calculate_extra_time(sender_processes)
            logger.info(f'Extra time spent on streaming: {extra_time}')

            logger.info('Done')
            time.sleep(3)

            if run_tshark:
                shared.cleanup_process(snd_tshark_process)
                time.sleep(3)
            if rcv == 'remotely':
                shared.cleanup_process(rcv_srt_process)
                time.sleep(3)

            if extra_time >= 5:
                logger.info(
                    f'Waited {config.time_to_stream + extra_time} seconds '
                    f'instead of {config.time_to_stream}. '
                    f'{bitrate}bps is considered as maximim available bandwidth.'
                )
                break
    except KeyboardInterrupt:
        logger.info('KeyboardInterrupt has been caught')
    except Exception as error:
        logger.info(
            f'Exception occured ({error.__class__.__name__}): {error}'
        )
    finally:
        logger.info('Cleaning up')

        if len(processes) == 0:
            logger.info('Nothing to clean up')
            return

        for process_tuple in reversed(processes):
            try:
                shared.cleanup_process(process_tuple)
            except shared.ProcessHasNotBeenKilled as error:
                # TODO: Collect the information regarding non killed processes
                # and perfom additional clean-up actions
                logger.info(
                    f'During cleaning up an exception occured '
                    f'({error.__class__.__name__}): {error}. The next '
                    f'experiment can not be done further!'
                )
                raise error


@click.command()
@click.argument(
    'config_filepath', 
    type=click.Path(exists=True)
)
@click.option(
    '--rcv', 
    type=click.Choice(['manually', 'remotely']), 
    default='remotely',
    help=	'Start a receiver manually or remotely via SSH. In case of '
            'manual receiver start, please do not forget to do it '
            'before running the script.',
    show_default=True
)
@click.option(
    '--snd-number', 
    default=1,
    help=   'Number of senders to start.',
    show_default=True
)
@click.option(
    '--snd-mode',
    type=click.Choice(['concurrently', 'parallel']), 
    default='concurrently',
    help=   'Start senders concurrently or in parallel.',
    show_default=True
)
@click.option(
    '--results-dir',
    default='_results',
    help=   'Directory to store results.',
    show_default=True
)
@click.option(
    '--collect-stats', 
    is_flag=True, 
    help='Collect SRT statistics.'
)
@click.option(
    '--run-tshark',
    is_flag=True,
    help='Run tshark.'
)
def main(
    config_filepath,
    rcv,
    snd_number,
    snd_mode,
    results_dir,
    collect_stats,
    run_tshark
):
    main_function(
        config_filepath,
        rcv,
        snd_number,
        snd_mode,
        results_dir,
        collect_stats,
        run_tshark
    )


if __name__ == '__main__':
    main()